{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59e063ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from env import TransitNetworkEnv\n",
    "from logger import TrainingEpisodeLogger\n",
    "from agent import Model, collect_rollout, ppo_update, fixed_policy\n",
    "from tqdm import tqdm\n",
    "from agent import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d102e44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def timetable_to_policy(env, timetable):\n",
    "    \"\"\"axis 0  is time and axis 1 is routes\"\"\"\n",
    "    expanded_timetable = np.zeros((int(env.transit_system_config[\"hours_of_opperation_per_day\"] * 3600\n",
    "                                  / env.transit_system_config[\"analysis_period_sec\"]),\n",
    "                                  len(env.possible_agents)))\n",
    "\n",
    "    for i in range(len(env.possible_agents)):\n",
    "        per_period_deployments = []\n",
    "        for j in range(env.transit_system_config[\"hours_of_opperation_per_day\"]):\n",
    "            deployments_per_hour = timetable[j, i]\n",
    "            expanded_deployments = np.zeros(int(3600/env.transit_system_config[\"analysis_period_sec\"]))\n",
    "            for _ in range(int(deployments_per_hour)):\n",
    "                ind = np.random.randint(0, expanded_deployments.shape[0])\n",
    "                while expanded_deployments[ind] != 0:\n",
    "                    ind = np.random.randint(0, expanded_deployments.shape[0])\n",
    "                expanded_deployments[ind] = 1\n",
    "            per_period_deployments.append(expanded_deployments)\n",
    "\n",
    "        expanded_timetable[:, i] = np.concatenate(per_period_deployments)\n",
    "    return expanded_timetable\n",
    "\n",
    "def policy_to_action_at_time(policy, t, s):\n",
    "    \"\"\"TxA\"\"\"\n",
    "    return {f\"agent_{i}\": a for i, a in enumerate(policy[int(t//s)])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21cabe7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_ftn(env, policy):\n",
    "    (\n",
    "        reward_buf,\n",
    "        terminated_buf,\n",
    "        truncated_buf,\n",
    "        info_buf,\n",
    "    ) = (\n",
    "        {agent_id: [] for agent_id in env.possible_agents},\n",
    "        {agent_id: [] for agent_id in env.possible_agents},\n",
    "        {agent_id: [] for agent_id in env.possible_agents},\n",
    "        {agent_id: [] for agent_id in env.possible_agents},\n",
    "    )\n",
    "\n",
    "    killed_agents = set()\n",
    "    sc = 0\n",
    "    num_killed = 0\n",
    "    for step_count in range(\n",
    "        int(\n",
    "            (env.transit_system_config[\"hours_of_opperation_per_day\"] * 3600)\n",
    "            / env.transit_system_config[\"analysis_period_sec\"]\n",
    "        )\n",
    "    ):\n",
    "\n",
    "        next_obs, reward, terminated, truncated, info = env.step(\n",
    "            policy_to_action_at_time(\n",
    "                policy,\n",
    "                env.current_time,\n",
    "                env.transit_system_config[\"analysis_period_sec\"],\n",
    "            )\n",
    "        )\n",
    "        for agent_id in env.possible_agents:\n",
    "            if agent_id not in killed_agents:\n",
    "                reward_buf[agent_id].append(\n",
    "                    torch.tensor(reward[agent_id], dtype=torch.float32)\n",
    "                )\n",
    "                info_buf[agent_id].append(info[agent_id])\n",
    "                terminated_buf[agent_id].append(terminated[agent_id])\n",
    "                truncated_buf[agent_id].append(truncated[agent_id])\n",
    "\n",
    "                if terminated[agent_id] or truncated[agent_id]:\n",
    "                    if agent_id not in killed_agents:\n",
    "                        killed_agents.add(agent_id)\n",
    "                if terminated[agent_id]:\n",
    "                    num_killed += 1\n",
    "                    sc += step_count\n",
    "\n",
    "        _ = next_obs\n",
    "        if len(killed_agents) == len(env.possible_agents):\n",
    "            break\n",
    "\n",
    "        good_buses = 0\n",
    "        for agent_id in env.possible_agents:\n",
    "            T = len(reward_buf[agent_id])\n",
    "            for t in reversed(range(T)):\n",
    "                current_time = info_buf[agent_id][t][\"current_time\"]\n",
    "                additional_reward = None\n",
    "                for i in range(t, T):\n",
    "                    retired_buses = info_buf[agent_id][i][\"retired_buses\"]\n",
    "                    for bus in retired_buses:\n",
    "                        if bus.created_at == current_time:\n",
    "                            if bus.num_passengers_served / bus.capacity > 0.90:\n",
    "                                additional_reward = 4\n",
    "                            elif bus.num_passengers_served / bus.capacity > 0.50:\n",
    "                                additional_reward = 2\n",
    "                            elif bus.num_passengers_served / bus.capacity > 0.10:\n",
    "                                additional_reward = 0\n",
    "                            elif bus.num_passengers_served / bus.capacity > 0.0:\n",
    "                                additional_reward = -2\n",
    "                            else:\n",
    "                                additional_reward = -4\n",
    "                            break\n",
    "\n",
    "                    if additional_reward is not None:\n",
    "                        reward_buf[agent_id][t] += additional_reward\n",
    "                        info_buf[agent_id][t][\"reward_type_3\"] += additional_reward\n",
    "                        good_buses += 1\n",
    "                        break\n",
    "\n",
    "    return np.array(\n",
    "        [\n",
    "            sum(\n",
    "                [info_buf[agent_id][t][\"reward_type_3\"] for t in range(policy.shape[0])]\n",
    "            )\n",
    "            + sum(\n",
    "                [info_buf[agent_id][t][\"reward_type_2\"] for t in range(policy.shape[0])]\n",
    "            )\n",
    "            for agent_id in env.possible_agents\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def run_simulated_anealing(seed=0):\n",
    "    env = TransitNetworkEnv({\"is_training\": True, \"seed\": seed})\n",
    "    _, _ = env.reset(hard_reset=True)\n",
    "\n",
    "    obj_history = []\n",
    "    timetable = (\n",
    "        np.ones(\n",
    "            (\n",
    "                env.transit_system_config[\"hours_of_opperation_per_day\"],\n",
    "                len(env.possible_agents),\n",
    "            )\n",
    "        )\n",
    "        * 1\n",
    "    )\n",
    "\n",
    "    old_objective = np.full(\n",
    "        len(env.possible_agents),\n",
    "        -np.inf,\n",
    "    )\n",
    "\n",
    "    final_timetable = timetable.copy()\n",
    "\n",
    "    for _ in (range(100)):\n",
    "        env = TransitNetworkEnv({\"is_training\": True, \"seed\": seed})\n",
    "        _, _ = env.reset(hard_reset=True)\n",
    "        policy = timetable_to_policy(env, timetable.copy())\n",
    "        objective = objective_ftn(env, policy)\n",
    "\n",
    "        for i in range(objective.shape[0]):\n",
    "            if objective[i] > old_objective[i]:\n",
    "                old_objective[i] = objective[i]\n",
    "                final_timetable[:, i] = timetable[:, i]\n",
    "            else:\n",
    "                timetable[:, i] = final_timetable[:, i]\n",
    "                \n",
    "        obj_history.append(old_objective.copy())\n",
    "        \n",
    "        for i in range(objective.shape[0]):\n",
    "            j = np.random.randint(timetable.shape[0])\n",
    "            timetable[j, i] += 1\n",
    "    \n",
    "    return final_timetable, obj_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334b5b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tts = {}\n",
    "for seed in [828920903,\n",
    "             115601515,\n",
    "             989005593,  \n",
    "             267626564, \n",
    "             913751312, \n",
    "             398943963, \n",
    "             436715962, \n",
    "             624311666, \n",
    "             433623492]:\n",
    "    \n",
    "    final_timetable, obj_history = run_simulated_anealing(seed=seed)\n",
    "    all_tts[seed] = final_timetable\n",
    "    print(seed, \"completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d670076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = TransitNetworkEnv({\"is_training\": True, \"seed\": 0}) \n",
    "# _, _ = env.reset(hard_reset=True)\n",
    "# t = env.transit_system.topology.show_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e84d78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "d[0].sum(axis=0).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ed0d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "d[-1][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67a724a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.array(d[-1])[:, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d50ad8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fr99",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
